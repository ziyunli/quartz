<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Before LLM  Until 2017, most NMT models, including the one that powered Google Translate, were recurrent neural networks. RNNs use Long Short-Term Memory (LSTM) cells to factor word order into their calculations."><meta property="og:title" content="Large Language Model"><meta property="og:description" content="Before LLM  Until 2017, most NMT models, including the one that powered Google Translate, were recurrent neural networks. RNNs use Long Short-Term Memory (LSTM) cells to factor word order into their calculations."><meta property="og:type" content="website"><meta property="og:image" content="https://ziyunli.github.io/quartz/icon.png"><meta property="og:url" content="https://ziyunli.github.io/quartz/notes/LLM/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Large Language Model"><meta name=twitter:description content="Before LLM  Until 2017, most NMT models, including the one that powered Google Translate, were recurrent neural networks. RNNs use Long Short-Term Memory (LSTM) cells to factor word order into their calculations."><meta name=twitter:image content="https://ziyunli.github.io/quartz/icon.png"><title>Large Language Model</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://ziyunli.github.io/quartz//icon.png><link href=https://ziyunli.github.io/quartz/styles.19109a40042e9f0e72e952fda4442a34.min.css rel=stylesheet><link href=https://ziyunli.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://ziyunli.github.io/quartz/js/darkmode.390405ce4fa88508cf8c697cadc4cf14.min.js></script>
<script src=https://ziyunli.github.io/quartz/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://ziyunli.github.io/quartz/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://ziyunli.github.io/quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://ziyunli.github.io/quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://ziyunli.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://ziyunli.github.io/quartz/",fetchData=Promise.all([fetch("https://ziyunli.github.io/quartz/indices/linkIndex.c4fbda67c7ba397e6c21d343796ea52a.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://ziyunli.github.io/quartz/indices/contentIndex.b6e78554a0a8abede5f72f418ef95bc7.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://ziyunli.github.io/quartz",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://ziyunli.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/ziyunli.github.io\/quartz\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=ziyunli.github.io/quartz src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://ziyunli.github.io/quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://ziyunli.github.io/quartz/>🪴 Ziyun's Backyard</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>Large Language Model</h1><p class=meta>Last updated
Jul 23, 2023
<a href=https://github.com/ziyunli/quartz/tree/hugo/content/notes/LLM.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#before-llm>Before LLM</a></li><li><a href=#bert-bidirectional-encoder-representations-from-transformers>BERT (Bidirectional Encoder Representations from Transformers)</a></li><li><a href=#gpt-generative-pretrained-transformer>GPT (Generative Pretrained Transformer)</a></li><li><a href=#local-llmnoteslocal20llmmd><a href=notes/Local%20LLM.md>Local LLM</a></a></li><li><a href=#applications>Applications</a><ol><li><a href=#langchain>LangChain</a></li></ol></li><li><a href=#prompt-engineeringnotesprompt20engineeringmd><a href=notes/Prompt%20Engineering.md>Prompt Engineering</a></a></li><li><a href=#prompt-injectionnotesprompt20injectionmd><a href=notes/Prompt%20Injection.md>Prompt Injection</a></a></li><li><a href=#fine-tuning>Fine-Tuning</a></li><li><a href=#ai-safety>AI Safety</a><ol><li><a href=#human-in-the-loop>Human in the Loop</a></li><li><a href=#typechattypechat><a href=TypeChat>TypeChat</a></a></li></ol></li></ol></nav></details></aside><a href=#before-llm><h2 id=before-llm><span class=hanchor arialabel=Anchor># </span>Before LLM</h2></a><blockquote><p>Until 2017, most NMT models, including the one that powered Google Translate, were 
<a href=https://en.wikipedia.org/wiki/Recurrent_neural_network rel=noopener>recurrent neural networks</a>. RNNs use 
<a href=https://en.wikipedia.org/wiki/Long_short-term_memory rel=noopener>Long Short-Term Memory</a> (LSTM) cells to factor word order into their calculations.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p></blockquote><blockquote><p>In 2017, a landmark paper titled “
<a href=https://arxiv.org/abs/1706.03762 rel=noopener>Attention Is All You Need</a>” changed the way data scientists approach NMT and other NLP tasks. That paper proposed a better way to process language based on <em>transformer models</em> that eschew LSTMs and use 
<a href=https://en.wikipedia.org/wiki/Attention_%28machine_learning%29 rel=noopener>neural attention</a> mechanisms to model the context in which words are used.</p></blockquote><p><img src=https://ziyunli.github.io/quartz//assets/language-model.png width=auto alt=language-model></p><p><img src=https://ziyunli.github.io/quartz//assets/language-model-2.png width=auto alt=language-model-2></p><p><img src=https://ziyunli.github.io/quartz//assets/language-model-3.png width=auto alt=language-model-3></p><a href=#bert-bidirectional-encoder-representations-from-transformers><h2 id=bert-bidirectional-encoder-representations-from-transformers><span class=hanchor arialabel=Anchor># </span>BERT (Bidirectional Encoder Representations from Transformers)</h2></a><blockquote><p>BERT isn’t generally useful by itself, but it can be fine-tuned to perform specific tasks such as sentiment analysis or question answering. Fine-tuning is accomplished by further training the pre-trained model with task-specific samples at a reduced learning rate, and it is <em>much</em> less expensive and time-consuming than training BERT from scratch.</p></blockquote><p><img src=https://ziyunli.github.io/quartz//assets/bert.png width=auto alt=bert></p><blockquote><p>Aside from the fact that it was trained with a huge volume of text, the key to BERT’s ability to understand human language is an innovation known as 
<a href=https://analyticsindiamag.com/a-complete-tutorial-on-masked-language-modelling-using-bert/ rel=noopener>Masked Language Modeling</a>. MLM turns a large corpus of text into a training ground for learning the structure of a language. When BERT models are pretrained, a specified percentage of the words in each batch of text – usually 15% – are randomly removed or “masked” so the model can learn to predict the missing words from the words around them. Unidirectional models look at the text to the left or the text to the right and attempt to predict what the missing word should be. MLM uses text on the left <em>and</em> right to inform its decisions.</p></blockquote><a href=#gpt-generative-pretrained-transformer><h2 id=gpt-generative-pretrained-transformer><span class=hanchor arialabel=Anchor># </span>GPT (Generative Pretrained Transformer)</h2></a><blockquote><p>Unlike BERT, GPT models can perform certain NLP tasks such as text translation and question-answering <em>without fine-tuning</em>, a feat known as <em>zero-shot</em> or <em>few-shot learning</em>.
&mldr;
ChatGPT is a fine-tuned version of GPT-3.5, which itself is a fined-tuned version of GPT-3. At its heart, ChatGPT is a transformer encoder-decoder that responds to prompts by iteratively predicting the first word in the response, then the second word, and so on.</p></blockquote><blockquote><p>The entire body of knowledge present on the Internet in September 2021 (and then some) was baked into those 175 billion parameters during training. It’s akin to you answering a question off the top of your head rather than reaching for your phone and Googling for an answer. When Microsoft incorporated GPT-4 into Bing, they added a separate layer providing Internet access.</p></blockquote><h2 id=local-llmnoteslocal20llmmd><a href=/quartz/notes/Local-LLM/ rel=noopener class=internal-link data-src=/quartz/notes/Local-LLM/>Local LLM</a></h2><a href=#applications><h2 id=applications><span class=hanchor arialabel=Anchor># </span>Applications</h2></a><blockquote><p>If you do understand this, then you have to ask, well, where are LLMs useful? Where is it useful to have automated undergraduates, or automated interns, who can repeat a pattern, that you might have to check?<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p></blockquote><a href=#langchain><h3 id=langchain><span class=hanchor arialabel=Anchor># </span>LangChain</h3></a><blockquote><p>&mldr;it makes it easy to call OpenAI’s GPT, say, a dozen times in a loop to answer a single question, and mix in queries to Wikipedia and other databases.
The clever bit is that, using LangChain, you <em>intercept</em> GPT when it starts a line with <em>“Act:”</em> and then you go and do that action for it, feeding the results back in as an <em>“Observation”</em> line so that it can “think” what to do next.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p></blockquote><h2 id=prompt-engineeringnotesprompt20engineeringmd><a href=/quartz/notes/Prompt-Engineering/ rel=noopener class=internal-link data-src=/quartz/notes/Prompt-Engineering/>Prompt Engineering</a></h2><h2 id=prompt-injectionnotesprompt20injectionmd><a href=/quartz/notes/Prompt-Injection/ rel=noopener class=internal-link data-src=/quartz/notes/Prompt-Injection/>Prompt Injection</a></h2><a href=#fine-tuning><h2 id=fine-tuning><span class=hanchor arialabel=Anchor># </span>Fine-Tuning</h2></a><p>How to generate training data
<a href="https://news.ycombinator.com/item?id=36070533" rel=noopener>https://news.ycombinator.com/item?id=36070533</a></p><ol><li>make your own RLHF dataset - like OpenAI and Open Assistant</li><li>exfiltrate data from a bigger/better LLM - Vicuna & family</li><li>use your pre-trained LLM to generate RLAIF data, no leeching - ConstitutionalAI, based on a set of rules instead of labelling examples</li></ol><p>How to use it with vector store
<a href="https://news.ycombinator.com/item?id=36070797" rel=noopener>https://news.ycombinator.com/item?id=36070797</a></p><blockquote><p>I would strongly recommend against fine-tuning over a set of documents as this is a very lossy information system retrieval system. LLMs are not well suited for information retrieval like databases and search engines.</p><p>The applications of fine-tuning that we are seeing have a lot of success is making completion models like LLaMA or original GPT3 become prompt-able. In essence, prompt-tuning or instruction-tuning. That is, giving it the ability to respond with a user prompt, llm output chat interface.</p><p>Vector databases, for now, are a great way to store mappings of embeddings of documents with the documents themselves for relevant-document information retrieval.</p></blockquote><p>Using vector store with fine-tuning
<a href="https://news.ycombinator.com/item?id=36079880" rel=noopener>https://news.ycombinator.com/item?id=36079880</a></p><blockquote><p>I want the user to be able to ask technical questions about a set of documents, then the user should retrieve a summary-answer from those documents along with a source.</p><p>I first need to fine-tune GPT4 so it better understands the niche-specific technical questions, the words used, etc. I could ask the fine-tuned model questions, but it won&rsquo;t really know from where it got the information. Without fine-tuning the summarized answer will suffer, or it will pull out the wrong papers.</p><p>Then I need to use a vector database to store the technical papers for the model to access; now I can ask questions, get a decent answer, and will have access to the sources.</p></blockquote><a href=#ai-safety><h2 id=ai-safety><span class=hanchor arialabel=Anchor># </span>AI Safety</h2></a><a href=#human-in-the-loop><h3 id=human-in-the-loop><span class=hanchor arialabel=Anchor># </span>Human in the Loop</h3></a><p>The danger of using MTurk as a &ldquo;human in the loop&rdquo; to fact-check LLM responses<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>:</p><blockquote><p>It is tempting to rely on crowdsourcing to validate LLM outputs or to create human gold-standard data for comparison. But what if crowd workers themselves are using LLMs, e.g., in order to increase their productivity, and thus their income, on crowdsourcing platforms?</p></blockquote><h3 id=typechattypechat><a href=/quartz/notes/TypeChat/ rel=noopener class=internal-link data-src=/quartz/notes/TypeChat/>TypeChat</a></h3><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=https://www.atmosera.com/ai/understanding-chatgpt/ rel=noopener>https://www.atmosera.com/ai/understanding-chatgpt/</a> &ldquo;Understanding ChatGPT&rdquo;&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a href=https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai rel=noopener>https://www.ben-evans.com/benedictevans/2023/7/2/working-with-ai</a> &ldquo;AI and the automation of work&rdquo;&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p><a href=https://interconnected.org/home/2023/03/16/singularity rel=noopener>https://interconnected.org/home/2023/03/16/singularity</a> &ldquo;The surprising ease and effectiveness of AI in a loop&rdquo;&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p><a href=https://techcrunch.com/2023/06/14/mechanical-turk-workers-are-using-ai-to-automate-being-human/ rel=noopener>https://techcrunch.com/2023/06/14/mechanical-turk-workers-are-using-ai-to-automate-being-human/</a> &ldquo;Mechanical Turk workers are using AI to automate being human&rdquo;&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz/notes/LLM101x/ data-ctx="Large Language Model" data-src=/notes/LLM101x class=internal-link>Large Language Models: Application through Production</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://ziyunli.github.io/quartz/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><script src=https://utteranc.es/client.js repo=ziyunli/quartz issue-term=pathname theme=boxy-light crossorigin=anonymous async></script><div id=contact_buttons><footer><p>Made by Ziyun Li using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://ziyunli.github.io/quartz/>Home</a></li><li><a href=https://fedi.ziyun.rocks/@ziyun>Mastodon</a></li><li><a href=https://github.com/ziyunli>GitHub</a></li><li><a href=https://blog.ziyun.rocks>Blog</a></li></ul></footer></div></div></body></html>