{"/":{"title":"ü™¥ Ziyun's Backyard","content":"\n\nHowdy!\n\nI am Ziyun Li(ÊùéÂ≠êÈõ≤), and I also go by Stephen. I am an electrical engineer turned software engineer, and currently working as a ~~Senior~~ **Staff Software Engineer** on  ~~Xbox One at [Microsoft](https://www.microsoft.com/)~~ ~~enterprise learning management system at [D2L](https://www.d2l.com/)~~ ~~classroom engagement software at [Top Hat](https://tophat.com/)~~ [API platform](https://docs.instacart.com/connect) at [Instacart](https://www.instacart.com/).\n\nInspired by [Derek Sivers](https://sive.rs/now), I have a [[notes/now|now]] page that tracks what I've been focused on at the moment.\n","lastmodified":"2023-07-02T21:15:07.640992358Z","tags":[]},"/notes/LLM101x":{"title":"Large Language Models: Application through Production","content":"\nI signed up [Large Language Models: Application through Production](https://learning.edx.org/course/course-v1:Databricks+LLM101x+2T2023/home) from Databricks on edX to learn more about applications of large language models. This page contains my notes on the course.\n\n\u003c!--more--\u003e\n\n# 0. Introduction\n\nThis module goes through some key concepts and terminology.\n\n- Language models: probabilistic models that assign probabilities to word sequences.\n- Large: from 10~50M to many billions of parameters. Made possible by transformer architecture since ~2017.\n\nPrimitives:\n- Token: basic building block of language models. Words, sub-words, characters, etc.\n- Sentence: sequence of tokens.\n- Vocabulary: complete list of tokens.\n\nTokenization:\n1. Words:\n   1. Intuitive\n   2. Big vocabulary\n   3. Complications such as misspelling, out-of-vocabulary (OOV) words, etc.\n2. Characters:\n   1. Small vocabulary\n   2. No OOV\n   3. Long sequences\n   4. No word-level semantics\n3. Subwords:\n  1. popular: byte pair encoding (BPE)\n\nWord embeddings:\n1. By frequency -\u003e sparsity issue\n2. word/token -\u003e embedding function -\u003e word embedding/vector\n\n# 1. Application\n\nThis module is in fact an introduction of huggingface transformers. The code examples are very straightforward to understand if you already know Python.\nIn fact the whole lab takes around 10 minutes to complete if it's not waiting to download all the data sets and models along the way.\n\nOn the high level, a HF pipeline could have these steps:\ninput -\u003e prompt constructions -\u003e tokenizer (encoding) -\u003e model -\u003e tokenizer (decoding) -\u003e output\n\nSome parameters to tweak:\n\ntokenizer:\n- `max_length`: max length of input sequence\n\nmodel:\n- `do_sample`: whether to use sampling\n  - `top_k`: top k tokens to sample from\n  - `top_p`: cumulative probability of top tokens to sample from\n  - `termperture`: temperature of sampling\n- `num_beams`: number of beams for beam search\n- `max_length`: max length of output sequence\n- `min_length`: min length of output sequence","lastmodified":"2023-07-02T21:15:07.640992358Z","tags":["LLM","GenAI"]},"/notes/Stable_Diffusion":{"title":"Stable Diffusion","content":"\n","lastmodified":"2023-07-02T21:15:07.640992358Z","tags":["StableDiffusion","GenAI"]},"/notes/learning_zig":{"title":"Learning Zig","content":"\n# Installation\n\nZig is moving fast, so I do not recommend installing it from the package manager (e.g. Homebrew, nix, etc.) as they are usually lagged.\nInstead, follow what [zigleanr](https://ziglearn.org/#installation) suggests, download the latest master build from https://ziglang.org/download/ and put it in your `$PATH`.\nNote that you want to put the entire directory in your `$PATH` instead of the binary itself; otherwise you will get an error like this:\n\n```\nerror: unable to find zig installation directory '/usr/local/bin/zig': FileNotFound\n```\n\n# Getting Started\n\n[ziglings](https://github.com/ziyunli/ziglings)\n\n# Built-in\n\nThe `@import()` function is built into Zig.\nIt returns a value which represents the imported code.\n\n```zig\nconst std = @import(\"std\");\n```\n\n# Types\n\n## Arrays\n\n*Is it allocated on the heap?*\n\nInitialization with `[n]type{ ... }`\n\n```zig\nvar foo = [_]u32{ 42, 108, 5423 };\n```\n\nConcatenate with `++`\n\n```zig\nvar foo = [_]u32{ 42, 108, 5423 } ++ [_]u32{ 42, 108, 5423 };\n```\n\nRepeat with `**`\n\n```zig\nvar foo = [_]u32{ 42, 108, 5423 } ** 2;\n```\n\n## Strings\n\nStrings use double quotes. Zig stores strings as arrays of bytes.\n\n```zig\nvar foo = \"Hello, world!\";\n```\n\nMulti-line string literals are supported with leading `\\\\` at the beginning of each line.\n\n```zig\nconst foo =\n  \\\\Hello,\n  \\\\world!\n```\n\n## Enums\n\n```zig\nconst Fruit = enum{ apple, pear, orange };\n```\n\n## Structs\n\n## Pointers\n\n\n\n# Conditions\n\n## If\n\n* If only takes boolean condition\n* If statements are valid expressions\n\n## While\n\nContinue expression is optional.\n\n```zig\nwhile (condition) : (continue expression) {\n  // ...\n}\n```\n\n`continue` and `break` are supported in while loops.\n\n## For\n\n`for` works like iterators on arrays and slices.\n\n```zig\nfor (items) |item, index| {\n  // ...\n}\n```\n\n## Switch\n\n`switch` works like `match` expression in other languages.\nSwitch statements must be \"exhaustive\".\nThey are also valid expressions.\n\n```zig\nswitch (players) {\n  1 =\u003e print(\"1 player\", .{}),\n  2 =\u003e print(\"2 players\", .{}),\n  else =\u003e print(\"{} players\", .{ players })\n}\n```\n\n`unreachable` is a special value that can be used to indicate that a switch statement is exhaustive.\n\n\n# Functions\n\nZig functions are private by default but the `main()` function should be public.\nA function is declared public with the `pub` statement.\n\n## Defer\n\n`defer` is a statement that executes a function when the current scope is exited.\n`errdefer` is a statement that executes a function when the current scope is exited with an error.\n\n\n# Errors\n\nErrors are created in \"error sets\", which works like an enum.\n\n```zig\nconst Error = error{ Foo, Bar };\n```\n\nZig support \"error unions\", which could be either a regular value OR an error from a set.\n\n```zig\nvar text: MyErrorSet!Text = ...;\n```\n\n`!void` will let Zig to infer the error type, which is useful for `main()`.\n\n## Error handling\n\n`catch` catches an error and replace it with a default value.\n\n```zig\nfoo = canFail() catch \"bar\"\n```\n\n`catch` can also capture the error and perform additional actions.\n\n```zig\nfoo = canFail() catch |err| {\n  if (err == FishError.TunaMalfunction) {\n    ...\n  }\n}\n```\n\n`try` is a shorthand for `catch` to return the error.\n\n```zig\ncanFail() catch |err| return err;\n\n// is equivalent to\n\ntry canFail();\n```\n\n`if` can be used to check if an error is present.\n\n```zig\nif (canFail()) |value| {\n  // ...\n} else |err| switch {\n  FishError.TunaMalfunction =\u003e ...,\n  else =\u003e ...,\n}\n```\n","lastmodified":"2023-07-02T21:15:07.640992358Z","tags":["Zig"]},"/notes/llm":{"title":"Teach Myself LLM","content":"\nA tracking page for my progress in learning LLM.\n\n### Deep learning\n* [x] [3Blue1Brown: Neural networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n- [ ] [fast.ai: Practical Deep Learning](https://course.fast.ai/)\n  - [x] [1: Getting started](https://course.fast.ai/Lessons/lesson1.html)\n  - [x] [2: Deployment](https://course.fast.ai/Lessons/lesson2.html)\n  - [x] [3: Neural net foundations](https://course.fast.ai/Lessons/lesson3.html)\n  - [ ] [4: Natural Language (NLP)](https://course.fast.ai/Lessons/lesson4.html)\n\n### Prompt engineering\n- [x] [DLAI: ChatGPT Prompt Engineering for Developers](https://learn.deeplearning.ai/chatgpt-prompt-eng)\n- [x] [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering/blob/main/README.md)\n- [ ] [Prompt Engineering Guide](https://www.promptingguide.ai/)\n- [ ] [Learn Prompting](https://learnprompting.org/)\n\n### Applications\n- [ ] [DLAI: Building Systems with the ChatGPT API](https://learn.deeplearning.ai/chatgpt-building-system)\n- [ ] [DLAI: LangChain for LLM Application Development](https://learn.deeplearning.ai/langchain)\n\n### Diffusion\n- [ ] [DLAI: How Diffusion Models Work](https://learn.deeplearning.ai/diffusion-models)\n\n### LLM\n- [ ] [LLM Introduction: Learn Language Models](https://gist.github.com/rain-1/eebd5e5eb2784feecf450324e3341c8d)\n- [ ] [edX: Large Language Models: Application through Production](https://learning.edx.org/course/course-v1:Databricks+LLM101x+2T2023/home)\n","lastmodified":"2023-07-02T21:15:07.640992358Z","tags":["LLM","GenAI"]},"/notes/now":{"title":"now","content":"\n- Ramping up on DL/AIGC\n\t- [ ] Taking [[notes/LLM101x|Large Language Models: Application through Production]]\n\t- Playing with [[notes/Stable_Diffusion|Stable Diffusion]]","lastmodified":"2023-07-02T21:15:07.640992358Z","tags":[]}}