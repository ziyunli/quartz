{"/":{"title":"ü™¥ Ziyun's Backyard","content":"\n\nHowdy!\n\nI am Ziyun Li(ÊùéÂ≠êÈõ≤), and I also go by Stephen. I am an electrical engineer turned software engineer, and currently working as a ~~Senior~~ **Staff Software Engineer** on  ~~Xbox One at [Microsoft](https://www.microsoft.com/)~~ ~~enterprise learning management system at [D2L](https://www.d2l.com/)~~ ~~classroom engagement software at [Top Hat](https://tophat.com/)~~ [API platform](https://docs.instacart.com/connect) at [Instacart](https://www.instacart.com/).\n\nInspired by [Derek Sivers](https://sive.rs/now), I have a [[notes/now|now]] page that tracks what I've been focused on at the moment.\n","lastmodified":"2023-07-02T21:02:48.581435728Z","tags":[]},"/notes/LLM101x":{"title":"Large Language Models: Application through Production","content":"\nI signed up [Large Language Models: Application through Production](https://learning.edx.org/course/course-v1:Databricks+LLM101x+2T2023/home) from Databricks on edX to learn more about applications of large language models. This page contains my notes on the course.\n\n\u003c!--more--\u003e\n\n# 0. Introduction\n\nThis module goes through some key concepts and terminology.\n\nLanguage models: probabilistic models that assign probabilities to word sequences.\nLarge: 10~50M to many billions of parameters. Made possible by transformer architecture since ~2017.\n\nToken: basic building block of language models. Word, subword, character, etc.\nSentence: sequence of tokens.\nVocabulary: complete list of tokens.\n\nTokenization:\n1. Words:\n   1. Intuitive\n   2. Big vocabulary\n   3. Complications such as misspelling, out-of-vocabulary (OOV) words, etc.\n2. Characters:\n   1. Small vocabulary\n   2. No OOV\n   3. Long sequences\n   4. No word-level semantics\n3. Subwords:\n  1. popular: byte pair encoding (BPE)\n\nWord embeddings:\n1. By frequency -\u003e sparsity issue\n2. word/token -\u003e embedding function -\u003e word embedding/vector\n\n# 1. Application\n\nThis module is in fact an introduction of huggingface transformers. The code examples are very straightforward to understand if you already know Python.\nIn fact the whole lab takes around 10 minutes to complete if it's not waiting to download all the data sets and models along the way.\n\nOn the high level, a HF pipeline could have these steps:\ninput -\u003e prompt constructions -\u003e tokenizer (encoding) -\u003e model -\u003e tokenizer (decoding) -\u003e output\n\nSome parameters to tweak:\n\ntokenizer:\n- `max_length`: max length of input sequence\n\nmodel:\n- `do_sample`: whether to use sampling\n  - `top_k`: top k tokens to sample from\n  - `top_p`: cumulative probability of top tokens to sample from\n  - `termperture`: temperature of sampling\n- `num_beams`: number of beams for beam search\n- `max_length`: max length of output sequence\n- `min_length`: min length of output sequence","lastmodified":"2023-07-02T21:02:48.581435728Z","tags":["Large Language Models","Generative AI"]},"/notes/Stable_Diffusion":{"title":"Stable Diffusion","content":"\n","lastmodified":"2023-07-02T21:02:48.581435728Z","tags":["Stable Diffusion","Generative AI"]},"/notes/now":{"title":"now","content":"\n- Ramping up on DL/AIGC\n\t- [ ] Taking [[notes/LLM101x|Large Language Models: Application through Production]]\n\t- Playing with [[notes/Stable_Diffusion|Stable_Diffusion]]","lastmodified":"2023-07-02T21:02:48.581435728Z","tags":[]}}